[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "Embracing the evolving landscape of disaster management, our project, Tweet Analytics for Disaster & Calamity Management, endeavors to curate critical and dependable insights as a beacon for those affected. Central to our endeavor is the differentiation between authentic and misleading tweets concerning calamities such as forest fires, earthquakes, and floods. Leveraging binary classification models and cutting-edge algorithms, we meticulously scrutinize live data streams, culminating in a comprehensive GUI that delivers actionable intelligence. Our mission is dedicated to furnishing trustworthy, real-time information to enhance the efficiency and efficacy of disaster response and management protocols."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "Embracing the evolving landscape of disaster management, our project, Tweet Analytics for Disaster & Calamity Management, endeavors to curate critical and dependable insights as a beacon for those affected. Central to our endeavor is the differentiation between authentic and misleading tweets concerning calamities such as forest fires, earthquakes, and floods. Leveraging binary classification models and cutting-edge algorithms, we meticulously scrutinize live data streams, culminating in a comprehensive GUI that delivers actionable intelligence. Our mission is dedicated to furnishing trustworthy, real-time information to enhance the efficiency and efficacy of disaster response and management protocols."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Introduction",
    "text": "Introduction\nAmidst the ever-evolving landscape of disasters, effective and prompt management necessitates innovative solutions leveraging the dynamic realm of social media. Our project stands at the forefront, addressing the challenges inherent in conventional crisis information gathering by harnessing the power of Twitter as a vital source of real-time data. With a proactive stance, we delve into tweet classification, employing techniques like tokenization to distill keywords related to disasters and non-disaster content, coupled with advanced models such as BERT and LSTM. Through data extraction from crucial topics like forest fires and earthquakes via Tweepy, our aim is to enrich a resilient disaster management strategy. The imminent integration of a user-friendly GUI interface will play a pivotal role in disseminating our predictions and outputs, culminating in a comprehensive and accessible tool for emergency responders and disaster management professionals."
  },
  {
    "objectID": "index.html#work-flow",
    "href": "index.html#work-flow",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Work Flow",
    "text": "Work Flow\n\nFake Tweet Classification\nWe performed classification of tweets into disaster-related and non-disaster-related categories using keyword count, character count, and word count distribution. This analysis served to enhance our understanding of the content and context of tweets during calamities.\n\n\nBERT Model Implementation\nDespite encountering resource constraints, we embarked on implementing a BERT (Bidirectional Encoder Representations from Transformers) model, achieving an accuracy of approximately 82% after three epochs. The BERT model is renowned for its contextual understanding and representation capabilities.\n\n\nLSTM Model Exploration\nIn response to resource limitations, we explored the implementation of an LSTM (Long Short-Term Memory) model, achieving a commendable accuracy of around 78%. The LSTM model, a type of recurrent neural network, demonstrated its effectiveness in capturing sequential dependencies in the data.\n\n\nData Extraction from Tweepy\nWe leveraged the Tweepy API to extract data pertaining to specific disaster topics, such as forest fires, floods, earthquakes, and hurricanes. This enabled us to gather real-time information and enhance the comprehensiveness of our analysis.\n\n\nGUI Interface Presentation\nAs part of our future work, we intend to present our predictions and outputs through an intuitive GUI (Graphical User Interface) interface. This interface will facilitate user-friendly access to our analytical insights, promoting effective decision-making in disaster and calamity management."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, our project endeavors to provide a robust and comprehensive solution to the challenges posed by dynamic disasters and calamities. By using models like BERT and LSTM"
  },
  {
    "objectID": "index.html#approach",
    "href": "index.html#approach",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Approach",
    "text": "Approach\n\nFake Tweet Classification\nOur approach involved a comprehensive classification of tweets, distinguishing between disaster-related and non-disaster-related content based on diverse metrics like keyword count, character count, and word count distributions. This meticulous analysis was instrumental in unraveling the nuances inherent in tweets during times of calamity, allowing for a more profound comprehension of their content and contextual relevance. To delve further into these insights, we constructed illuminating word clouds for both disaster and non-disaster tweets. These visual representations encapsulated the most prevalent words in each category, offering a vivid portrayal of the prevalent themes and discourse within the realm of calamitous events.\n\n\nTokenization\nTokenization is the process of breaking down text into smaller units, known as tokens. These tokens can be individual words or subwords, phrases, or even characters, depending on the specific tokenization technique used. The primary goal of tokenization is to facilitate natural language processing tasks by converting textual data into a format that can be easily handled by algorithms and models.\nUtilizing the Tokenizer() function from the Keras library in Python, we meticulously tokenized our textual data with a specific vocabulary size set at 1000. The outcome of this process yielded a list of sequences, where each sequence encapsulated the transformation of textual content into a sequence of integers. This transformation paved the way for a structured and numerical representation of the textual information, laying the groundwork for subsequent analysis and modeling endeavors.\n\n\nBERT Model Implementation\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is a pre-trained language representation model developed by Google. It‚Äôs designed to understand the context of words in a sentence by capturing bidirectional relationships in the text.\nKey features of BERT:\n\nBidirectional Encoding: BERT considers the full context of a word by looking at both the left and right context in all layers of the model. This bidirectional approach helps in understanding the nuances of language.\nPre-trained Model: BERT is pre-trained on large corpora of text, learning to predict missing words in a sentence, which enables it to capture intricate relationships between words.\nFine-tuning: BERT‚Äôs pre-trained weights can be fine-tuned on specific tasks by adding additional layers or training it on domain-specific data. This allows BERT to be adapted to various natural language processing (NLP) tasks like text classification, named entity recognition, question answering, and more.\nEmbeddings: BERT generates contextual word embeddings that capture the meaning of a word based on its context in a sentence. These embeddings are used as features for downstream NLP tasks.\n\nWe tried to implement a BERT-based binary classification model, achieving an accuracy of approximately 82% after three epochs. The BERT model is renowned for its contextual understanding and representation capabilities. Due to lack of computational resources we had to abort the model and moved ahead with a different approach.\n\n\nLSTM Model Exploration\nLong Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture designed to handle the issue of vanishing or exploding gradients in traditional RNNs. LSTMs are capable of capturing long-term dependencies in sequential data by using a memory cell that can maintain information over long sequences.\nKey features of LSTM:\n\nMemory Cells: LSTMs use memory cells to store information, which allows them to remember and access information from earlier time steps in a sequence.\nGates: LSTMs contain three gates: input gate, forget gate, and output gate. These gates regulate the flow of information into and out of the memory cell, enabling the network to learn when to forget or update information.\nLong-Term Dependencies: LSTMs excel in capturing and retaining information over longer sequences, making them suitable for tasks involving sequential data like text, time series, speech, etc.\nTraining: LSTMs are trained using backpropagation through time (BPTT) and can be optimized using various gradient descent algorithms like Adam, RMSprop, etc.\n\nIn response to resource limitations, we explored the implementation of an LSTM (Long Short-Term Memory) model, achieving a commendable accuracy of around 82%. The LSTM model, a type of recurrent neural network, demonstrated its effectiveness in capturing sequential dependencies in the data.\n\n\nData Extraction from ntscraper\nThe Ntscraper library is a user-friendly tool tailored for effortless data retrieval from Nitter instances. It empowers users to extract tweets efficiently through a range of functionalities:\n\nSearch and Scraping Capabilities: Easily locate and extract tweets containing specific terms or hashtags, streamlining the process of aggregating targeted content.\nUser Profile Scraping: Seamlessly retrieve tweets from user profiles, enabling a focused exploration of individual user activity.\nUser Profile Information: Fetch pertinent profile details like the display name, username, tweet count, and profile picture, offering comprehensive insights into user profiles.\n\nFurthermore, in the absence of a specified instance, the scraper automatically selects a random public instance, ensuring a seamless scraping experience regardless of the instance availability.\nUtilizing the robust capabilities of the ntscraper API, our initiative focused on extracting critical data related to distinct disaster topics, encompassing forest fires, floods, earthquakes, and hurricanes. By harnessing this API, we accessed real-time and contextually relevant information, significantly enriching the depth and breadth of our analysis.\nThis strategic utilization empowered our project to stay dynamically aligned with ongoing events and swiftly gather comprehensive insights crucial for in-depth examination and effective disaster management strategies.\n\n\nGUI Interface Presentation (Streamlit)\nStreamlit is an open-source Python library that simplifies the process of creating web applications for data science and machine learning projects. It allows developers and data scientists to build interactive and customizable web-based applications using simple Python scripts.\nKey features of Streamlit:\n\nEasy-to-Use: Streamlit provides a straightforward and intuitive API that allows users to create web applications using familiar Python scripting.\nRapid Prototyping: With Streamlit, you can quickly create interactive dashboards, visualizations, and applications by leveraging its built-in widgets and components.\nIntegration: It seamlessly integrates with popular data science libraries like Pandas, Matplotlib, Plotly, and more, enabling easy incorporation of data manipulation and visualization capabilities into web apps.\nReal-Time Updates: Streamlit‚Äôs reactive framework automatically updates the app in response to changes in the underlying data or user inputs, providing a dynamic and responsive user experience.\nDeployment: Once the application is built, it was deployed upon Streamlit‚Äôs own cloud sharing platform.\n\nWe intend to present our predictions and outputs through an intuitive GUI (Graphical User Interface) interface. This interface will facilitate user-friendly access to our analytical insights, promoting effective decision-making in disaster and calamity management. For developing GUI we use streamlit."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "Unleashing the Power of Twitter to strengthen Disaster Response and safeguard public safety during Critical Events."
  },
  {
    "objectID": "proposal.html#high-level-goal",
    "href": "proposal.html#high-level-goal",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "Unleashing the Power of Twitter to strengthen Disaster Response and safeguard public safety during Critical Events."
  },
  {
    "objectID": "proposal.html#goal-description-motivation",
    "href": "proposal.html#goal-description-motivation",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Goal Description & Motivation:",
    "text": "Goal Description & Motivation:\nSocial media platforms like Twitter provide an unprecedented opportunity to access a vast amount of information and gain insights directly from those affected by disasters. By analyzing the conversations and trends on Twitter during such situations, the project aims to uncover patterns, validate the truthfulness of information, improve situational awareness, and ultimately contribute to the resilience and safety of individuals and communities faced with critical events.\nThe major motivation behind the proposed project is to save lives and improve public safety by harnessing the power of Twitter data to access real-time information, understand & validate public sentiment."
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Dataset",
    "text": "Dataset\nThis dataset has been taken from Natural Language Processing with Disaster Tweets competition. Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they‚Äôre observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e.¬†disaster relief organizations and news agencies).\nThere are two main files associated with this dataset:\n\ntrain.csv - credibility ratings and reasoning entered by turkers** for each topic. Each row has following tab separated fields:\nData Glimpse\n\n\n\n\n\n\n\nid\n\n\nkeyword\n\n\nlocation\n\n\ntext\n\n\ntarget\n\n\n\n\n\n\n66\n\n\nablaze\n\n\nGREENSBORO,NORTH CAROLINA\n\n\nHow the West was burned: Thousands of wildfire‚Ä¶\n\n\n1\n\n\n\n\n\n\n\nData Description\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\n\n\nid\nInteger\nA unique identifier for each tweet\n\n\ntext\nList of strings\nIt contains the text of the tweet\n\n\nlocation\nList of strings\nThe location the tweet was sent from (may be blank)\n\n\nkeyword\nList of Strings\nA particular keyword from the tweet (may be blank)\n\n\ntarget\nInteger\nin¬†train.csv¬†only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n\n\nThis data set will be used to train the model on the credibility of the tweet\ntest.csv - tweets corresponding to each topic fetched using the search API. Each row has following tab separated fields:\nData Description\nThis dataset will be used as an index to fetch and test specific types of disaster tweets (like earthquake).\n\nFor streaming API, we will be using tweepy, a python package meand for extracting real-time tweets using keywords. It will be authenticated using a developer account from the v2 API endpoint. This endpoint will serve on a free tier to fetch and consume streaming data from twitter (https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data). The individual data streamed by this API are JSON encoded, and fall into the following types:\n\nTweets: Individual Tweet JSON objects\nKeep-alive Signals: Carriage returns to prevent your connection from timing out\nSystem messages: E.g. notification of a force disconnect. Note that the actual disconnection is accomplished via normal HTTP protocols, rather than through the message itself. In some cases the disconnect system message may not arrive, making it critical that you monitor the keep-alive signal (see below for more information).\n\nWe will be utilizing the Tweet JSON (shown below) to analyze and classify them into real or fake. This can be particularly useful for disaster management departments, as the analysis of these tweets can help extract crucial information, such as the geolocation of the tweet, which can be used to send help to those in need.\n\n\nExpand to reveal Tweet JSON output\n{\n  \"created_at\": \"Thu Apr 06 15:24:15 +0000 2017\",\n  \"id_str\": \"850006245121695744\",\n  \"text\": \"1\\/ Today we\\u2019re sharing our vision for the future of the Twitter API platform!\\nhttps:\\/\\/t.co\\/XweGngmxlP\",\n  \"user\": {\n    \"id\": 2244994945,\n    \"name\": \"Twitter Dev\",\n    \"screen_name\": \"TwitterDev\",\n    \"location\": \"Internet\",\n    \"url\": \"https:\\/\\/dev.twitter.com\\/\",\n    \"description\": \"Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\\/\\/twittercommunity.com\\/ \\u2328\\ufe0f #TapIntoTwitter\"\n  },\n  \"place\": {   \n  },\n  \"entities\": {\n    \"hashtags\": [      \n    ],\n    \"urls\": [\n      {\n        \"url\": \"https:\\/\\/t.co\\/XweGngmxlP\",\n        \"unwound\": {\n          \"url\": \"https:\\/\\/cards.twitter.com\\/cards\\/18ce53wgo4h\\/3xo1c\",\n          \"title\": \"Building the Future of the Twitter API Platform\"\n        }\n      }\n    ],\n    \"user_mentions\": [     \n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\nKey Name\nData Type\nDescription\n\n\n\n\ncreated_at\nString\nUTC time when this Tweet was created.\n\n\nid_str\nString\nThe string representation of the unique identifier for this Tweet.\n\n\ntext\nString\nThe actual UTF-8 text of the status update.\n\n\nuser | User object | The user who posted this Tweet. See User data dictionary for complete list of attributes.\n\n\n\n\nplace\nPlaces Object\nWhen present, indicates that the tweet is associated (but not necessarily originating from) a Place.\n\n\nentities\nEntities Object\nEntities which have been parsed out of the text of the Tweet. Additionally see Entities in Twitter Objects. Example:\n{ ‚Äúhashtags‚Äù:[],\n‚Äúurls‚Äù:[],\n‚Äúuser_mentions‚Äù:[],\n‚Äúmedia‚Äù:[],\n‚Äúsymbols‚Äù:[]\n‚Äúpolls‚Äù:[] }\n\n\n\n\n\nThe combination of these two sources provides a rich and comprehensive data set for your application. The credibility annotations in the CREDBANK data can aid in the fake/real classification, while the academictwitteR package can help in collecting real-time data for ongoing analysis."
  },
  {
    "objectID": "proposal.html#reason-for-choosing-this-source",
    "href": "proposal.html#reason-for-choosing-this-source",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Reason For Choosing This Source",
    "text": "Reason For Choosing This Source\nThese dataset‚Äôs not only provide a large volume of data but also offer unique features that align well with our project‚Äôs objectives. The combination of historical data from¬†CREDBANK¬†and real-time data from¬†academictwitteR¬†can provide a robust foundation for the tweet analysis application."
  },
  {
    "objectID": "proposal.html#problem-statement",
    "href": "proposal.html#problem-statement",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe dynamic nature of disasters and calamities demands a proactive approach to information gathering and analysis. Traditional methods often fail to adapt quickly to evolving situations, hindering the ability of emergency responders to anticipate needs and allocate resources effectively. Furthermore, ensuring the reliability and credibility of the information shared on Twitter during these events is crucial, as misinformation and rumors can lead to panic and confusion among the affected population."
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Analysis Plan",
    "text": "Analysis Plan\nFor the project, we are going to split into weekly milestones and attack the project in a piece by piece method. The goal is to build on the project an deploy a fully functional real-time classifier from the Twitter API. The plan is split into 6 weeks, with each of the key milestones described as follows:\nWeek 1:\nAvikal and Himanshu:\n\nDataset Exploration\n\nExtract characteristics from the dataset, like UserID, Message Content, Location, etc.\nConvert to Binary Classes.\n\n\nShakir and Poojitha: - Data Preprocessing\n\nNull value and Duplicate value treatment.\nText Cleaning (removing special characters, stop words).\nSplit dataset into training and testing.\n\nVisalakshi, Himanshu and Avikal: - Feature Extraction\n\nTokenization.\nStemming.\nLemmatization.\n\nAll members: - Classification\n\nTrain and validate the model.\nTest on new tweets.\n\nWeek 2:\n\nAvikal, Visalakshi and Shakir: Extract entities from the tweet that include information about geo-location, disaster-type, severity, time elapsed, etc. using NER techniques.\nHimanshu and Poojitha: Link the entities to tweets and save the information.\n\nWeek 3:\n\nVisalakshi and Avikal: Use python Tweepy to extract information and setup data flow.\nShakir: Define filtering parameters to separate disaster-related tweets.\nPoojitha and Himanshu: Process and store the incoming tweet data along with the extracted results from the tweet data.\n\nWeek 4:\n\nEveryone: Visualize the extracted output in a GUI. This will include showing geographic locations of the affected regions.\nEveryone: Add a dashboard to this GUI that provides better analysis of the extracted information.\n\nWeek 5:\n\nAvika;, Shakir and Visalakshi: Create an interface for the analysis dashboard.\nVisalakshi and Himanshu: Use maps to indicate affected locations.\nPoojitha: Generate intelligent analytics on the type of disaster, time elapsed, severity, etc.\n\nWeek 6:\n\nEveryone: Deploy the GUI and the data.\nEveryone: Make changes according to feedback.\nEveryone: Make the final presentation for showcase.\n\n\n\n\n\n\n\nNote:\n\n\n\nThese are the planned approaches, and we intend to explore and solve the problem statement which we came up with. Parts of our approach might change in the final project."
  },
  {
    "objectID": "proposal.html#plan-of-attack",
    "href": "proposal.html#plan-of-attack",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Plan of Attack",
    "text": "Plan of Attack\n\n\n\n\n\n\n\nWeek\nWeekly Task\n\n\n\n\nWeek 1\nProposal.\n\n\nWeek 2\nData Exploration and Preprocessing.\n\n\nWeek 3 - Week 4\nWorking on setting up the API and Extracting Feature using methods like Tokenization, lemmatization etc. and train Models and perform prediction.\n\n\nWeek 5\nVisualize the output by working on developing a GUI.\n\n\nWeek 6\nFinalizing the project and complete preparing a presentation.\n\n\n\n** We have added respective members for individual tasks in the Analysis Plan. We have removed Members Responsible Column from the table."
  },
  {
    "objectID": "proposal.html#repo-organization",
    "href": "proposal.html#repo-organization",
    "title": "üïäÔ∏è Tweet Analytics for Disaster & Calamity Management",
    "section": "Repo Organization",
    "text": "Repo Organization\n\n.github: This directory contain files related to GitHub, such as workflows, issue templates, or other configurations.\n. _extra: Contains code, notes and other files used during experimentation. Contents of this folder is not a part of the final output.\n_freeze: The folder created to store files generated during project render.\nanalysis/: This folder contains the analysis scripts used to generate output for the project outline.\n\nREADME.md describes the steps to run and generate the results using scripts\nOther code files are added to review and understand the source of the output\nRelevant datasets used for the scripts are under the data/ folder in the main directory.\n\ndata/: This folder contains data files or datasets that are used in the project.\n\nREADME.md : A readme file that describes the datasets in more detail.\n\nimages: This folder contains image files that are used in the project, such as illustrations, diagrams, or other visual assets.\n.gitignore: This file specifies which files or directories should be ignored by version control.\nREADME.md: This file usually contains documentation or information about the project. It‚Äôs often the first thing someone reads when they visit the project repository.\n_quarto.yml: This is likely a configuration file\nabout.qmd : This quarto document contains the information about team members.\nindex.qmd : This quarto document contains the approach and analysis and results of the project.\npresentation.qmd : It contains the slides for the presentation.\nproposal.qmd : This quarto documents has the proposal of the project.\nproject-final.Rproj : This is an RStudio project file, which helps organize R-related files and settings for the project."
  },
  {
    "objectID": "analysis/Twitter_API_test.html",
    "href": "analysis/Twitter_API_test.html",
    "title": "With Twitter API",
    "section": "",
    "text": "tweepy with oauth v2 app_only\n\nimport tweepy\n\nbearer_token=\"---\"\naccess_token=\"---\"\naccess_token_secret=\"---\"\nconsumer_key = \"---\"\nconsumer_secret = \"---\"\n\nclient_id = \"---\"\nclient_secret = \"---\"\n\n\nclient = tweepy.Client(consumer_key=consumer_key,\n                       consumer_secret=consumer_secret,\n                       access_token=access_token,\n                       access_token_secret=access_token_secret,\n                       bearer_token=bearer_token\n                       )\n\n\nquery = '#forestfire -is:retweet lang:en'\n\n\nclient.get_me()\n\nResponse(data=&lt;User id=1725708459348500480 name=visalakshi iyer username=visa_algo1&gt;, includes={}, errors=[], meta={})\n\n\n\n\n# Authenticate with Twitter\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth)\n\n# Define your search query\nsearch_query = 'your_search_query'\n\n\n\n\n# Fetch tweets based on the search query\ntweets = tweepy.Cursor(api.search_tweets, q=query, lang='en').items(10)\n\n\n\n# tweets = api.search_tweets(q=query, lang=\"en\", count=10, tweet_mode ='extended')\n\n\nNitter Scraper\n\n!pip install ntscraper\n\n\nfrom ntscraper import Nitter\nimport pandas as pd\nimport numpy as np\n\n\nscraper = Nitter()\n\nTesting instances: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:38&lt;00:00,  1.20s/it]\n\n\n\ntweets_forest = scraper.get_tweets(terms='forestfire', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')\n\n\ntweets_forest\n\n\ndef save_tweets(tweet_dict, df = None, topic=\"None\"):\n    if df is None:\n        df =  pd.DataFrame(columns=['link', 'text', 'user', 'date', 'is_retweet', 'comments', 'retweets', 'quotes', 'likes', 'images', 'video', 'gifs', \"topic\"])\n\n    for tweet in tweet_dict['tweets']:\n        images_ = [tweet['pictures'][0] if tweet['pictures'] != [] else np.nan]\n        videos_ = [tweet['videos'][0] if tweet['videos'] != [] else np.nan]\n        gifs_ = [tweet['gifs'][0] if tweet['gifs'] != [] else np.nan]\n        df = df.append(pd.DataFrame([[tweet[\"link\"],\n                tweet[\"text\"],\n                tweet[\"user\"][\"name\"],\n                tweet['date'],\n                tweet['is-retweet'],\n                tweet['stats']['comments'],\n                tweet['stats']['retweets'],\n                tweet['stats']['quotes'],\n                tweet['stats']['likes'],\n                images_[0],\n                videos_[0],\n                gifs_[0],\n                topic\n                    ]], columns = df.columns))\n\n    df.to_csv(\"/content/saved_tweets.csv\", index=False)\n\n    return df\n\n\n\n\ndf = save_tweets(tweets_forest, topic=\"#forestfire\")\n\n\ndf\n\n\n  \n    \n\n\n\n\n\n\nlink\ntext\nuser\ndate\nis_retweet\ncomments\nretweets\nquotes\nlikes\nimages\nvideo\ngifs\ntopic\n\n\n\n\n0\nhttps://twitter.com/caryinstitute/status/17291...\n#jobopportunity Program Manager for the Wester...\nCary Institute\nNov 27, 2023 ¬∑ 2:54 PM UTC\nFalse\n0\n1\n0\n2\nNaN\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/Irisheyes_1114/status/1699...\nThe smoke is unreal this morning #forestfire #...\nJenny Düë©üèª‚Äç‚öïÔ∏èüçÄ\nSep 7, 2023 ¬∑ 12:12 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/BrianEdwardRoth/status/169...\nVolunteering in Ukraine. Supporting Internatio...\nBrian Edward Roth\nAug 28, 2023 ¬∑ 3:50 PM UTC\nFalse\n1\n1\n0\n3\nNaN\nhttps://video.twimg.com/ext_tw_video/169618846...\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/CoastalCalm_BC/status/1693...\nMowat Bay park, British Columbia, Canada, so s...\nShane Greaves\nAug 22, 2023 ¬∑ 3:44 AM UTC\nFalse\n0\n0\n0\n2\nhttps://pbs.twimg.com/media/F4GyxIoa4AA362k.jpg\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/YouTravel/status/169337870...\nDuring the Emergency Services #forestfire #eva...\nUrsula Maxwell Travels\nAug 20, 2023 ¬∑ 9:45 PM UTC\nFalse\n0\n0\n0\n1\nNaN\nNaN\nNaN\n#forestfire\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0\nhttps://twitter.com/DonNeigel/status/129656689...\nEnjoying a beer at @HatcheryBrewing in #pentic...\nbeer traveler\nAug 20, 2020 ¬∑ 9:56 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/AlbertaSnowbird/status/129...\nSmoke blowing in or storm? 19:00 looking west ...\nSnowbirdsInTraining\nAug 20, 2020 ¬∑ 1:04 AM UTC\nFalse\n0\n0\n0\n0\nhttps://pbs.twimg.com/media/Ef02EszUcAA9xeW.jpg\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/mcbean409/status/129590385...\nCan't watch #DNC2020 because of local fires an...\nDanny Garibay\nAug 19, 2020 ¬∑ 2:02 AM UTC\nFalse\n0\n0\n0\n1\nNaN\nNaN\nhttps://video.twimg.com/tweet_video/Efv5kCfVAA...\n#forestfire\n\n\n0\nhttps://twitter.com/MistisaMaria/status/129579...\nThe calm before the storm. So grateful for th...\nMaria Baula- Schuh\nAug 18, 2020 ¬∑ 6:38 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/larry_bud/status/129546497...\nBecause of I-70 being closed the detour west i...\nLarry Budwig\nAug 17, 2020 ¬∑ 8:58 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#forestfire\n\n\n\n\n\n200 rows √ó 13 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntweets_earthquake = scraper.get_tweets(terms='earthquake', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')\n\nINFO:root:No instance specified, using random instance https://nitter.moomoo.me\nINFO:root:Current stats for earthquake: 11 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 31 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 44 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 64 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 80 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 95 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 109 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 128 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 148 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 167 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 182 tweets, 0 threads...\nINFO:root:Current stats for earthquake: 200 tweets, 0 threads...\n\n\n\ndf = save_tweets(tweets_forest, df=df, topic=\"#earthquake\")\n\n\ndf\n\n\ntweets_floods = scraper.get_tweets(terms='floods', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')\n\nINFO:root:No instance specified, using random instance https://nitter.io.lol\nINFO:root:Current stats for floods: 12 tweets, 0 threads...\nINFO:root:Current stats for floods: 29 tweets, 0 threads...\nINFO:root:Current stats for floods: 45 tweets, 0 threads...\nINFO:root:Current stats for floods: 59 tweets, 0 threads...\nINFO:root:Current stats for floods: 77 tweets, 0 threads...\nINFO:root:Current stats for floods: 97 tweets, 0 threads...\nINFO:root:Current stats for floods: 114 tweets, 0 threads...\nINFO:root:Current stats for floods: 129 tweets, 0 threads...\nINFO:root:Current stats for floods: 144 tweets, 0 threads...\nINFO:root:Current stats for floods: 161 tweets, 0 threads...\nINFO:root:Current stats for floods: 177 tweets, 0 threads...\nINFO:root:Current stats for floods: 194 tweets, 0 threads...\nINFO:root:Current stats for floods: 200 tweets, 0 threads...\n\n\n\ndf = save_tweets(tweets_forest, df=df, topic=\"#floods\")\n\n\ntweets_hurricane = scraper.get_tweets(terms='hurricane', mode='hashtag', number=200, near='usa', exclude=['nativeretweets'], language='en')\n\nINFO:root:No instance specified, using random instance https://nitter.uni-sonia.com\nINFO:root:Current stats for hurricane: 15 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 35 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 52 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 69 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 88 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 105 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 124 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 144 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 163 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 175 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 192 tweets, 0 threads...\nINFO:root:Current stats for hurricane: 200 tweets, 0 threads...\n\n\n\ndf = save_tweets(tweets_forest, df=df, topic=\"#hurricane\")\n\n\ndf\n\n\n  \n    \n\n\n\n\n\n\nlink\ntext\nuser\ndate\nis_retweet\ncomments\nretweets\nquotes\nlikes\nimages\nvideo\ngifs\ntopic\n\n\n\n\n0\nhttps://twitter.com/caryinstitute/status/17291...\n#jobopportunity Program Manager for the Wester...\nCary Institute\nNov 27, 2023 ¬∑ 2:54 PM UTC\nFalse\n0\n1\n0\n2\nNaN\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/Irisheyes_1114/status/1699...\nThe smoke is unreal this morning #forestfire #...\nJenny Düë©üèª‚Äç‚öïÔ∏èüçÄ\nSep 7, 2023 ¬∑ 12:12 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/BrianEdwardRoth/status/169...\nVolunteering in Ukraine. Supporting Internatio...\nBrian Edward Roth\nAug 28, 2023 ¬∑ 3:50 PM UTC\nFalse\n1\n1\n0\n3\nNaN\nhttps://video.twimg.com/ext_tw_video/169618846...\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/CoastalCalm_BC/status/1693...\nMowat Bay park, British Columbia, Canada, so s...\nShane Greaves\nAug 22, 2023 ¬∑ 3:44 AM UTC\nFalse\n0\n0\n0\n2\nhttps://pbs.twimg.com/media/F4GyxIoa4AA362k.jpg\nNaN\nNaN\n#forestfire\n\n\n0\nhttps://twitter.com/YouTravel/status/169337870...\nDuring the Emergency Services #forestfire #eva...\nUrsula Maxwell Travels\nAug 20, 2023 ¬∑ 9:45 PM UTC\nFalse\n0\n0\n0\n1\nNaN\nNaN\nNaN\n#forestfire\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0\nhttps://twitter.com/DonNeigel/status/129656689...\nEnjoying a beer at @HatcheryBrewing in #pentic...\nbeer traveler\nAug 20, 2020 ¬∑ 9:56 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#hurricane\n\n\n0\nhttps://twitter.com/AlbertaSnowbird/status/129...\nSmoke blowing in or storm? 19:00 looking west ...\nSnowbirdsInTraining\nAug 20, 2020 ¬∑ 1:04 AM UTC\nFalse\n0\n0\n0\n0\nhttps://pbs.twimg.com/media/Ef02EszUcAA9xeW.jpg\nNaN\nNaN\n#hurricane\n\n\n0\nhttps://twitter.com/mcbean409/status/129590385...\nCan't watch #DNC2020 because of local fires an...\nDanny Garibay\nAug 19, 2020 ¬∑ 2:02 AM UTC\nFalse\n0\n0\n0\n1\nNaN\nNaN\nhttps://video.twimg.com/tweet_video/Efv5kCfVAA...\n#hurricane\n\n\n0\nhttps://twitter.com/MistisaMaria/status/129579...\nThe calm before the storm. So grateful for th...\nMaria Baula- Schuh\nAug 18, 2020 ¬∑ 6:38 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#hurricane\n\n\n0\nhttps://twitter.com/larry_bud/status/129546497...\nBecause of I-70 being closed the detour west i...\nLarry Budwig\nAug 17, 2020 ¬∑ 8:58 PM UTC\nFalse\n0\n0\n0\n0\nNaN\nNaN\nNaN\n#hurricane\n\n\n\n\n\n800 rows √ó 13 columns"
  },
  {
    "objectID": "analysis/Data_Mining_LSTM.html",
    "href": "analysis/Data_Mining_LSTM.html",
    "title": "Tokenizing the text",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy\n\n\ntrain_data = pd.read_csv(\"train.csv\")\ntest_data = pd.read_csv(\"test.csv\")\n\n\nX = train_data['text'].tolist()\ny = train_data['target'].tolist()\n\n\nmax = 1000\ntokenizer = Tokenizer(num_words = max, oov_token='&lt;OOV&gt;')\ntokenizer.fit_on_texts(X)\nsequence = tokenizer.texts_to_sequences(X)\n\n\nmaxlen = 100\ndataX =pad_sequences(sequence, maxlen=maxlen)\n\n\ny = np.array(y)\n\n\nX_train, X_val, y_train, y_val = train_test_split(dataX, y, test_size=0.2, random_state=42)\n\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max, output_dim=128, input_length=maxlen))\nmodel.add(LSTM(units=64))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4,verbose=1,restore_best_weights = True)\n\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n\nEpoch 1/20\n191/191 [==============================] - 22s 98ms/step - loss: 0.5492 - accuracy: 0.7169 - val_loss: 0.4720 - val_accuracy: 0.7892\nEpoch 2/20\n191/191 [==============================] - 16s 83ms/step - loss: 0.4155 - accuracy: 0.8136 - val_loss: 0.4557 - val_accuracy: 0.7945\nEpoch 3/20\n191/191 [==============================] - 16s 85ms/step - loss: 0.3817 - accuracy: 0.8371 - val_loss: 0.4720 - val_accuracy: 0.8004\nEpoch 4/20\n191/191 [==============================] - 16s 85ms/step - loss: 0.3605 - accuracy: 0.8470 - val_loss: 0.4808 - val_accuracy: 0.7807\nEpoch 5/20\n191/191 [==============================] - 17s 91ms/step - loss: 0.3392 - accuracy: 0.8571 - val_loss: 0.5078 - val_accuracy: 0.7866\nEpoch 6/20\n191/191 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.8632Restoring model weights from the end of the best epoch: 2.\n191/191 [==============================] - 16s 86ms/step - loss: 0.3201 - accuracy: 0.8632 - val_loss: 0.5215 - val_accuracy: 0.7768\nEpoch 6: early stopping\n\n\n&lt;keras.src.callbacks.History at 0x7b615907a740&gt;\n\n\n\nmodel.save('trained_lstm_model.h5')\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model("
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by The AlgoRhythms For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr.¬†Greg Chism. The team is comprised of the following team members.\n\nVisalakshi Iyer: First-Year Graduate student pursuing M.S. in Data Science at University of Arizona.\nAvikal Singh: First-Year Graduate student pursuing M.S. in Information Science at University of Arizona.\nShakir Ahmed: First-Year Graduate student pursuing M.S. in Data Science at University of Arizona.\nPoojitha Pasala: First-Year Graduate student pursuing M.S. in Data Science at University of Arizona.\nHimanshu Nimbarte: First-Year Graduate student pursuing M.S. in Data Science at University of Arizona."
  },
  {
    "objectID": "analysis/DM_Classification_Pipeline_Training.html",
    "href": "analysis/DM_Classification_Pipeline_Training.html",
    "title": "Project Title",
    "section": "",
    "text": "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenize as tkn_module\nfrom sklearn.model_selection import train_test_split\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('fivethirtyeight')\n\nData load:\n\ntrain_data = pd.read_csv(\"/content/sample_data/train.csv\")\ntest_data = pd.read_csv(\"/content/sample_data/test.csv\")\n\ntrain_data.head()\n\n\n  \n    \n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n1\nNaN\nNaN\nOur Deeds are the Reason of this #earthquake M...\n1\n\n\n1\n4\nNaN\nNaN\nForest fire near La Ronge Sask. Canada\n1\n\n\n2\n5\nNaN\nNaN\nAll residents asked to 'shelter in place' are ...\n1\n\n\n3\n6\nNaN\nNaN\n13,000 people receive #wildfires evacuation or...\n1\n\n\n4\n7\nNaN\nNaN\nJust got sent this photo from Ruby #Alaska as ...\n1\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nVisualizing data\n\nprint(\"Shape of the training dataset: {}.\".format(train_data.shape))\nprint(\"Shape of the testing dataset: {}\".format(test_data.shape))\nfor col in train_data.columns:\n    nan_vals = train_data[col].isna().sum()\n    pcent = (train_data[col].isna().sum() / train_data[col].count()) * 100\n    print(\"Total NaN values in column '{}' are: {}, which is {:.2f}% of the data in that column\".format(col, nan_vals, pcent))\n\nShape of the training dataset: (7613, 5).\nShape of the testing dataset: (3263, 4)\nTotal NaN values in column 'id' are: 0, which is 0.00% of the data in that column\nTotal NaN values in column 'keyword' are: 61, which is 0.81% of the data in that column\nTotal NaN values in column 'location' are: 2533, which is 49.86% of the data in that column\nTotal NaN values in column 'text' are: 0, which is 0.00% of the data in that column\nTotal NaN values in column 'target' are: 0, which is 0.00% of the data in that column\n\n\n\n# Let's plot NaN value distribution\nfig = sns.barplot(\n    x=train_data[['keyword', 'location']].isna().sum().index,\n    y=train_data[['keyword', 'location']].isna().sum().values,\n)\n\n\n\n\nvisual of target values\n\nvals = [len(train_data[train_data['target']==1]['target']), len(train_data[train_data['target']==0]['target'])]\n\nplt.pie(vals, labels=[\"Non-Disaster\", \"Disaster\"])\nplt.axis('equal')\nplt.title(\"Target Value Distribution\")\nplt.show()\n\n\n\n\nkeyword frequency count\n\nfig = plt.figure(figsize=(10, 70), dpi=100)\nsns.countplot(y=train_data['keyword'].sort_values(), hue=train_data['target'])\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\nfig.show()\n\n\n\n\ncharachter count\n\ndis_twt = train_data[train_data['target']==1]['text'].str.len()\nnon_dis_twt = train_data[train_data['target']==0]['text'].str.len()\n\nsns.displot([dis_twt, non_dis_twt])\n\n\n\n\nword count distribution\n\ndis_cnt = train_data[train_data['target'] == 1]['text'].str.split().map(lambda x: len(x))\nndis_cnt = train_data[train_data['target'] == 0]['text'].str.split().map(lambda x: len(x))\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=list(dis_cnt), name='Disaster Tweets'),\n    row=1,\n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(ndis_cnt), name='Non Disaster Tweets'),\n    row=1,\n    col=2,\n)\n\nfig.update_layout(height=500, width=950, title_text=\"Words Count\")\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\naverage word length\n\ndis_avg = train_data[train_data['target']==1]['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\nndis_avg = train_data[train_data['target']==0]['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\n\nfig = ff.create_distplot([dis_avg, ndis_avg], ['Disaster', 'Non Disaster'])\nfig.update_layout(height=500, width=950, title_text=\"Average Word Length Distribution\")\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\nunique word count distribution\n\ndis_uvc = train_data[train_data['target']==1]['text'].apply(lambda x: len(set(str(x).split()))).to_list()\nndis_uvc = train_data[train_data['target']==0]['text'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([dis_uvc, ndis_uvc], ['Disaster', 'Non Disaster'])\nfig.update_layout(height=500, width=950, title_text=\"Unique Word Count Distribution\")\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\nURL count\n\ndis_uc = train_data[train_data['target']==1]['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w])).to_list()\nndis_uc = train_data[train_data['target']==0]['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w])).to_list()\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=dis_uc, name='Disaster Tweets'),\n    row=1,\n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=ndis_uc, name='Non Disaster Tweets'),\n    row=1,\n    col=2,\n)\n\nfig.update_layout(height=500, width=950, title_text=\"URL Count\")\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\nword cloud for diaster and non disaster tweets\n\ndis_snt = train_data[train_data['target']==1]['text'].to_list()\ndis_snt = \" \".join(dis_snt)\n\ndis_wc = WordCloud(width=256, height=256, collocations=False).generate(dis_snt)\nplt.figure(figsize = (7,7))\nplt.imshow(dis_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()\n\n\n\n\n\nndis_snt = train_data[train_data['target']==0]['text'].to_list()\nndis_snt = \" \".join(ndis_snt)\n\nndis_wc = WordCloud(width=256, height=256, collocations=False).generate(ndis_snt)\nplt.figure(figsize = (7,7))\nplt.imshow(ndis_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()\n\n\n\n\nMODELS: Fine tuned BERT model\n\n# Load the BERT model from TensorFlow Hub\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable=True)\n\n# Load the BERT tokenizer from Hugging Face Transformers\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens, all_masks, all_segments = [], [], []\n\n    for text in tqdm(texts):\n        # Tokenize the current text\n        text = tokenizer.tokenize(text)\n        # Select text only till\n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n\n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ngetting model from TFHub\nGet the model from TFHub and the vocab file with it\n\n%%time\nurl = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(url, trainable=True)\n\nCPU times: user 13 s, sys: 3.65 s, total: 16.7 s\nWall time: 16.4 s\n\n\n\n# Print the type of the tokenizer before calling bert_encode\nprint(type(tokenizer))\n\n&lt;class 'transformers.models.bert.tokenization_bert.BertTokenizer'&gt;\n\n\nencoding the data\n\n%%time\ntrain_input = bert_encode(train_data['text'].values, tokenizer, max_len=200)\ntest_input = bert_encode(test_data['text'].values, tokenizer, max_len=200)\ntrain_labels = train_data['target'].values\n\n\n\n\n\n\n\nCPU times: user 9.52 s, sys: 65.8 ms, total: 9.59 s\nWall time: 10.4 s\n\n\nbuilding the model\n\ndef build_model(transformer, max_len=512):\n    # Naming your keras ops is very important üòâ\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n    # Get the sequence output\n    _, seq_op = transformer([input_word_ids, input_mask, segment_ids])\n    # Get the respective class token from that sequence output\n    class_tkn = seq_op[:, 0, :]\n    # Final Neuron (for Classification)\n    op = Dense(1, activation='sigmoid')(class_tkn)\n    # Bind the inputs and outputs together into a Model\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=op)\n\n    model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model\n\ntraining the model:\n\n# Build the model\nmodel = build_model(bert_layer, max_len=200)\nmodel.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_word_ids (InputLayer  [(None, 200)]                0         []                            \n )                                                                                                \n                                                                                                  \n input_mask (InputLayer)     [(None, 200)]                0         []                            \n                                                                                                  \n segment_ids (InputLayer)    [(None, 200)]                0         []                            \n                                                                                                  \n keras_layer_1 (KerasLayer)  [(None, 1024),               3351418   ['input_word_ids[0][0]',      \n                              (None, 200, 1024)]          89         'input_mask[0][0]',          \n                                                                     'segment_ids[0][0]']         \n                                                                                                  \n tf.__operators__.getitem (  (None, 1024)                 0         ['keras_layer_1[0][1]']       \n SlicingOpLambda)                                                                                 \n                                                                                                  \n dense (Dense)               (None, 1)                    1025      ['tf.__operators__.getitem[0][\n                                                                    0]']                          \n                                                                                                  \n==================================================================================================\nTotal params: 335142914 (1.25 GB)\nTrainable params: 335142913 (1.25 GB)\nNon-trainable params: 1 (1.00 Byte)\n__________________________________________________________________________________________________\n\n\n\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.1,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=8\n)"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "",
    "text": "In times of disasters, individuals frequently turn to social media platforms to communicate information regarding required aid or incidents.\nTwitter provides an unprecedented opportunity to access a vast amount of information and gain insights directly from those affected by disasters. Harnessing of Twitter‚Äôs potential to is crucial to enhance disaster response and ensure public safety in critical situations."
  },
  {
    "objectID": "presentation.html#introduction",
    "href": "presentation.html#introduction",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Introduction:",
    "text": "Introduction:\n\nIn times of disasters, individuals frequently turn to social media platforms to communicate information regarding required aid or incidents.\nTwitter provides an unprecedented opportunity to access a vast amount of information and gain insights directly from those affected by disasters. Harnessing of Twitter‚Äôs potential is crucial to enhance disaster response and ensure public safety in critical situations."
  },
  {
    "objectID": "presentation.html#goal-and-problem-statement",
    "href": "presentation.html#goal-and-problem-statement",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Goal and problem statement:",
    "text": "Goal and problem statement:\n\nGoal- In the face of dynamic disasters, a proactive approach to information is essential. Traditional methods can lag, hampering swift responses and resource allocation. Validating Twitter-shared information is critical to prevent panic and confusion among affected communities.\nProblem statement- Conduct classification on tweets during crisis, validate tweets, and deliver accurate information to disaster management teams, thereby contributing to the smooth functioning of rescue operations and boosting the safety and resilience of communities during critical events."
  },
  {
    "objectID": "presentation.html#data",
    "href": "presentation.html#data",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Data:",
    "text": "Data:\n\nTwitter API enables developers to fetch tweets in real-time using keywords. Since the previous month, twitter had revoked the access to pull tweets in free tier.\nTo get the Real-time tweets, we resorted to creating and utilizing a Nitter instance from a web-scraping based package. The tweets are fetched based on type of disaster (hashtag) and location.\nThese tweets will be then analysed in real-time, annotated with insights generated from our modeling process\nThe modeling required Fake Tweet Classification dataset from kaggle is used for classification of tweets as real or fake.\nThe model trained on this dataset was utilized for classifying and segregating the tweets to filter out spotlight tweets.\n\nOur analysis plan involved exploring the dataset for suitability of the training. The insights will be shown in the next slides"
  },
  {
    "objectID": "presentation.html#execution-of-plan",
    "href": "presentation.html#execution-of-plan",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Execution of plan:",
    "text": "Execution of plan:\n\nCreated a new streaming pipeline- Used NTscraper to create a Nitter object which uses Beautiful soup in the back end to fetch tweet details such as ‚Äútext‚Äù, ‚Äúusername‚Äù and statistics such as ‚Äúlikes‚Äù,‚Äúcomments‚Äù and ‚Äúretweets‚Äù.\nModel building- Used LSTM model to build a fake news classification model and give us the relevant and important details corresponding to disaster management.\nModel training- Used Kaggle dataset to perform model training and get accuracy scores.\nModel testing- Use real time data based on hashtag- ‚Äú#forestfire‚Äù and location set to near ‚ÄúUSA‚Äù.\nGUI creation- Integrate a search bar for input of hashtag, after which the model fetches the data and performs classification. The classified real tweets are displayed with tweet link, location (if available) and the user."
  },
  {
    "objectID": "presentation.html#results",
    "href": "presentation.html#results",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Results:",
    "text": "Results:\nhash_earthquake:"
  },
  {
    "objectID": "presentation.html#live-demo",
    "href": "presentation.html#live-demo",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Live Demo:",
    "text": "Live Demo:\nNow, we are going to demonstrate our project in action.\nGUI for Disaster Tweet Analysis"
  },
  {
    "objectID": "presentation.html#conclusion",
    "href": "presentation.html#conclusion",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nIt has been a constant effort of the developer community to utilize twitter and generate analysis to manage/monitor disasters. However, there are some challenges when considering social media as an information source for disaster response. In particular, social media streams contain large amounts of irrelevant messages such as rumors, advertisements, or even misinformation.\nThis project is an improvement to previously attempted disaster tweet monitoring systems hosted by Google (Tensorflow and Kaggle competitions). What we achieved on top of the previous attempts, is the improved model scores and higher analytic insights on the location tracking.\nThis particular attempt can serve as a derivation for improve intelligence received during a crisis, and can enable intelligent officers to access calls for help quickly.\n\n\n\n\nüïä The Algo-Rhythms"
  },
  {
    "objectID": "presentation.html#results-1",
    "href": "presentation.html#results-1",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Results:",
    "text": "Results:\nhash_flood:"
  },
  {
    "objectID": "presentation.html#results-2",
    "href": "presentation.html#results-2",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Results:",
    "text": "Results:\nhash_hurricane:"
  },
  {
    "objectID": "presentation.html#pie-chart",
    "href": "presentation.html#pie-chart",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Pie Chart:",
    "text": "Pie Chart:\n\n\nThis is the target variable distribution of training dataset. With a fairly even distribution, our model had a high chance of reliable training outcomes."
  },
  {
    "objectID": "presentation.html#bar-plot",
    "href": "presentation.html#bar-plot",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Bar Plot:",
    "text": "Bar Plot:\n\n\nThe main unique keywords that fall under the tweets tagged as ‚Äúdisaster tweets‚Äù has the following distribution. It is evident that tweets that talk about the disaster have really focuses terms according to the disaster."
  },
  {
    "objectID": "presentation.html#density-plot",
    "href": "presentation.html#density-plot",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Density Plot:",
    "text": "Density Plot:\n\n\nWith both distribution of the categories not being significantly different from an ideal normal distribution, we had the notion that the model will receive sufficient learning tokens"
  },
  {
    "objectID": "presentation.html#word-cloud",
    "href": "presentation.html#word-cloud",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Word Cloud:",
    "text": "Word Cloud:\n\n\nThis is the wordcloud from the combined tweets of the dataset, for the ‚Äúdisaster‚Äù class."
  },
  {
    "objectID": "presentation.html#bert",
    "href": "presentation.html#bert",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "BERT",
    "text": "BERT\n\nFirst the dataset tweets were broken down through tokenization. These tokens can be individual words or subwords, phrases, or even characters, depending on the specific tokenization technique used. The primary goal of tokenization is to facilitate natural language processing tasks by converting textual data into a format that can be easily handled by algorithms and models.\nTraining BERT was computationally expensive, and well as memory intensive. The accuracy score for BERT was 97% on training and 82% on testing."
  },
  {
    "objectID": "presentation.html#lstm",
    "href": "presentation.html#lstm",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "LSTM",
    "text": "LSTM\n\nLSTM on the other hand was not memory intensive, although it was fairly heavy to train, it was able to achieve a great learning curve. It achieved a training accuracy of 83% and testing accuracy of 80%.\nWhen both the models were tested on the real-time set, LSTM provided better results compared to BERT. Owing to the fact that some fake tweets which contain the main keywords, can be misclassified due to the attention mechanism."
  },
  {
    "objectID": "presentation.html#challenges-faced",
    "href": "presentation.html#challenges-faced",
    "title": "Tweet Analytics for Disaster and Calamity Management",
    "section": "Challenges Faced",
    "text": "Challenges Faced\n\nWe were not able to get access to fetch tweets with twitter developer API, because of the revoked access of free tier developers. This was later improvised using a web scraping object\nThere were a greater need for computational power, as text based models are computationally heavy. We utilized online resources and GPU to train the model and load them.\nResponse time is a greater requirement when it comes to tools like disaster monitoring systems. With heavy models running in the background, memory optimization and caching plays a great role."
  }
]