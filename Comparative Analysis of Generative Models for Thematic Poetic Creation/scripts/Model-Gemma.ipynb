{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90a1760-c57c-405c-81a7-ecb429c4c68f",
   "metadata": {},
   "source": [
    "# Poem Generation using gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2c149-5f86-43cb-a52e-da6f5ae767f0",
   "metadata": {},
   "source": [
    "### Load Model and Generate Sonnets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f879d16-787e-4fc6-88cf-2af1675b6a01",
   "metadata": {},
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Provide your Hugging Face token\n",
    "login(token=\"xxxxxxxxxxxxxxxxxxxxxxxx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0a268d9c-80dd-4c8a-924b-f943f055750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.util import ngrams\n",
    "import pronouncing\n",
    "from nltk.corpus import cmudict\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b30f96f8-5394-4ad3-b5f3-c111225881ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ec510102f444fc8de7abfd7fc56cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68763ba5-6eea-4ee3-af91-9b840632cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem 1:\n",
      " Write a Shakespearean sonnet about courage set in a battlefield with a determined tone. Use vivid imagery to convey strength and resilience.\n",
      "\n",
      "Upon the field of strife, where blood doth stain,\n",
      "And steel meets steel in clashing, deadly dance,\n",
      "A heart of valor beats, unyielding, plain,\n",
      "A warrior's soul, with fire in its trance.\n",
      "\n",
      "Though fear may whisper, doubt may take its hold,\n",
      "And shadows of despair may darken the mind,\n",
      "Yet courage, like a beacon, brightly bold,\n",
      "Will pierce the darkness, leave no soul behind.\n",
      "\n",
      "With every step, a testament to might,\n",
      "Each fallen foe, a victory to claim,\n",
      "The spirit unbent, a beacon in the night,\n",
      "A warrior's will, a burning, endless flame.\n",
      "\n",
      "So let the battle rage, let blood run free,\n",
      "For courage, like a river, flows eternally. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the poem\n",
    "input_text = \"Write a Shakespearean sonnet about courage set in a battlefield with a determined tone. Use vivid imagery to convey strength and resilience.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate poem\n",
    "outputs = model.generate(**input_ids, max_new_tokens=350, temperature=0.7, top_p=0.9)\n",
    "generated_poem1 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Poem 1:\\n\", generated_poem1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b6b7bb5-5e2e-46b6-ad47-ad40dfb27184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem 2:\n",
      " Write a Shakespearean sonnet about wonder set in outer space with an awe-filled tone. Use vivid imagery to convey mystery and discovery.\n",
      "\n",
      "Upon the velvet canvas of the night,\n",
      "A million diamonds, scattered, bright and bold,\n",
      "The Milky Way, a river of pure light,\n",
      "A story whispered, centuries old.\n",
      "And in that vast expanse, where shadows lie,\n",
      "A tapestry of stars, a cosmic dance,\n",
      "Each twinkling point, a mystery to spy,\n",
      "A silent symphony, a timeless trance.\n",
      "The planets spin, in orbits grand and wide,\n",
      "Their fiery breath, a cosmic fire's glow,\n",
      "And in their depths, secrets yet to hide,\n",
      "A universe of wonder, we all know.\n",
      "So let us gaze, with hearts both pure and free,\n",
      "And marvel at the majesty, eternally. \n",
      "\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Upon the velvet canvas of the night,\n",
      "A million diamonds, scattered, bright and bold,\n",
      "The Milky Way, a river of pure light,\n",
      "A story whispered, centuries old.\n",
      "And in that vast expanse, where shadows lie,\n",
      "A tapestry of stars, a cosmic dance,\n",
      "Each twinkling point, a mystery to spy,\n",
      "A silent symphony, a timeless trance.\n",
      "The planets spin, in orbits grand and wide,\n",
      "Their fiery breath, a cosmic fire's glow,\n",
      "And in their depths, secrets yet to hide,\n",
      "A universe of wonder, we all know.\n",
      "So let us gaze, with hearts both pure and free,\n",
      "And marvel at the majesty, eternally. \n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The sonnet uses vivid imagery and a formal structure to evoke a sense of wonder and awe. \n",
      "\n",
      "* **Imagery:** The poem uses metaphors like \"velvet canvas of the night\" and \"a tapestry of stars\" to create a visual picture\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the poem\n",
    "input_text = \"Write a Shakespearean sonnet about wonder set in outer space with an awe-filled tone. Use vivid imagery to convey mystery and discovery.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate poem\n",
    "outputs = model.generate(**input_ids, max_new_tokens=350, temperature=0.7, top_p=0.9)\n",
    "generated_poem2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Poem 2:\\n\", generated_poem2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ba10e9bb-f91c-4d09-ae27-64be784e4a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem 3:\n",
      " Write a Shakespearean sonnet about loss set in an empty home with a somber tone. Use vivid imagery to convey grief and reflection.\n",
      "\n",
      "The dust motes dance in sunbeams, pale and thin,\n",
      "Across the empty hall, a silent scene.\n",
      "The scent of lavender, a memory's spin,\n",
      "A phantom touch, a love that's never been.\n",
      "\n",
      "The worn rug, once vibrant, now lies still,\n",
      "A tapestry of faded, broken hues.\n",
      "The fireplace, cold, a hollow, silent chill,\n",
      "Where laughter once did fill the room with news.\n",
      "\n",
      "The clock ticks slow, a mournful, steady beat,\n",
      "Each second echoes the void within.\n",
      "The house, a tomb, where love and life did meet,\n",
      "Now echoes only with the wind's lament.\n",
      "\n",
      "And in this silence, I find my own despair,\n",
      "A heart that aches, a soul beyond repair. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the poem\n",
    "input_text = \"Write a Shakespearean sonnet about loss set in an empty home with a somber tone. Use vivid imagery to convey grief and reflection.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate poem\n",
    "outputs = model.generate(**input_ids, max_new_tokens=350, temperature=0.7, top_p=0.9)\n",
    "generated_poem3 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Poem 3:\\n\", generated_poem3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28821aaa-95f2-475f-a37d-1d1cbad550dc",
   "metadata": {},
   "source": [
    "#### Extracting the necessary content (sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5a7c942a-36df-4b63-b9ec-779a6de97f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Poem_Data.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f16cf113-a7a8-4ac2-81fc-f23828c81420",
   "metadata": {},
   "outputs": [],
   "source": [
    "poem1 = data.loc[0, 'Gemma 2 2B Q4']\n",
    "poem2 = data.loc[1, 'Gemma 2 2B Q4']\n",
    "poem3 = data.loc[2, 'Gemma 2 2B Q4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3a6b5-0b3e-4b28-93fa-1032866def79",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525a43d-f05b-40c5-bcf2-82245126675b",
   "metadata": {},
   "source": [
    "#### BLEU (Bilingual Evaluation Understudy)\n",
    "##### What it does: \n",
    "BLEU measures the precision of n-grams (sequences of n consecutive words) in the generated text when compared to a reference text. It’s widely used for machine translation but can also apply to poetry.\n",
    "##### How we’re using it: \n",
    "While BLEU is typically associated with translation, we use it here to evaluate how well the n-grams of the generated poems align with reference poems. This gives us a basic sense of similarity between generated and target structures in terms of word patterns.\n",
    "\n",
    "#### METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "##### What it does: \n",
    "METEOR considers semantic and syntactic similarity by accounting for synonyms, stemming, and word order. This allows it to address some of the weaknesses of the BLEU metric.\n",
    "##### How we’re using it: \n",
    "We use METEOR to assess how closely the generated poem matches the meaning and structure of a reference poem. This is especially helpful for understanding if the model maintains context or theme throughout the text.\n",
    "\n",
    "#### Perplexity\n",
    "##### What it does: \n",
    "Perplexity measures how well a language model can predict the next word in a sequence. A lower perplexity means the model has more confidence in its predictions, which can translate to better fluency.\n",
    "##### How we’re using it: \n",
    "We use perplexity to gauge the fluency and coherence of the generated poems. It gives us an idea of how \"natural\" the poem sounds, even though it doesn’t capture all aspects of creativity.\n",
    "\n",
    "#### Entropy\n",
    "##### What it does: \n",
    "Entropy measures the randomness or diversity of a text by quantifying how varied the token choices are in a given sequence. Higher entropy suggests the model is generating more diverse content, while lower entropy signals repetition or predictability.\n",
    "##### How we’re using it: \n",
    "Entropy is important for understanding how creative or structured the generated poetry is. A balance is key—moderate entropy indicates diversity and creativity, while very low or very high entropy might signal overly repetitive or chaotic outputs. We use this metric to compare different models and analyze their ability to create engaging, varied poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cdd60ed2-66e9-4e9e-8451-c0a045e19148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "def bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate the BLEU score between the reference and candidate texts.\n",
    "    Measures the overlap of n-grams between the generated text and the reference.\n",
    "    Returns the score rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    return round(sentence_bleu([reference.split()], candidate.split()), 5)\n",
    "\n",
    "def meteor_score_func(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate the METEOR score between the reference and candidate texts.\n",
    "    METEOR measures both exact matches and semantically similar matches \n",
    "    (stemming or synonyms) and penalizes overly short outputs.\n",
    "    Tokenizes both inputs before computing the score.\n",
    "    Returns the score rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    # Tokenize both the reference and candidate\n",
    "    reference_tokens = reference.split()  # Tokenize the reference poem\n",
    "    candidate_tokens = candidate.split()  # Tokenize the generated poem\n",
    "    return round(meteor_score([reference_tokens], candidate_tokens), 5)\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a text using the provided tokenizer and model.\n",
    "    Perplexity is a measure of how well the language model predicts the text.\n",
    "    Lower perplexity indicates higher fluency and alignment with the model's training data.\n",
    "    - Uses the tokenizer to convert the text into input tensors.\n",
    "    - Performs inference using the model with no gradient calculation (for efficiency).\n",
    "    - Computes the loss from the model's output and converts it to perplexity.\n",
    "    Returns the perplexity value rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")  # Tokenize text into tensors\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])  # Compute loss\n",
    "        loss = outputs.loss  # Extract loss\n",
    "        return round(torch.exp(loss).item(), 5)  # Convert loss to perplexity\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a text, representing the diversity in its character or word usage.\n",
    "    Entropy quantifies the unpredictability of the text.\n",
    "    - Tokenizes the text\n",
    "    - Counts the occurrences of each token.\n",
    "    - Calculates probabilities for each token and uses them to compute entropy.\n",
    "    Returns the entropy value rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    tokens = list(text)  # Tokenize the text at the character level (default)\n",
    "    token_counts = Counter(tokens)  # Count occurrences of each token\n",
    "    total_tokens = len(tokens)  # Total number of tokens\n",
    "    probabilities = [count / total_tokens for count in token_counts.values()]  # Compute probabilities\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities)  # Compute entropy\n",
    "    return round(entropy, 5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "db0ee1f9-04de-4d0c-82b0-c7e051d57c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_to_evaluate = \"Gemma 2 2B Q4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f7cec7af-c6a0-4c72-a9fa-881549af6f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for Model: Gemma 2 2B Q4\n",
      "   Prompt   METEOR  Perplexity  Entropy\n",
      "0       1  0.08862     3.12202  4.40724\n",
      "1       2  0.14262     3.64469  4.43066\n",
      "2       3  0.06653     3.94740  4.38255\n",
      "\n",
      "Average Entropy: 4.4068\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Scores\n",
    "results = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    prompt = row[\"PromptID\"]\n",
    "    reference_poem = row[\"ReferencePoem\"]\n",
    "    generated_poem = row[model_name_to_evaluate]\n",
    "    \n",
    "    # Calculate scores\n",
    "    #bleu = bleu_score(reference_poem, generated_poem)\n",
    "    meteor = meteor_score_func(reference_poem, generated_poem)\n",
    "    perplexity = calculate_perplexity(generated_poem)\n",
    "    entropy = calculate_entropy(generated_poem)\n",
    "    \n",
    "    # Append to results\n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        #\"BLEU\": bleu,\n",
    "        \"METEOR\": meteor,\n",
    "        \"Perplexity\": perplexity,\n",
    "        \"Entropy\": entropy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "average_entropy = results_df[\"Entropy\"].mean()\n",
    "evaluation_title = f\"Evaluation Results for Model: {model_name_to_evaluate}\"\n",
    "print(evaluation_title)\n",
    "print(results_df)\n",
    "print(f\"\\nAverage Entropy: {average_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b59c08-e4f2-4bbc-b666-4a20d2026825",
   "metadata": {},
   "source": [
    "##### METEOR Scores:\n",
    "* The METEOR scores (0.08862, 0.14262, 0.06653) are relatively low, suggesting that the generated poems exhibit limited lexical and semantic overlap with the reference poems.\n",
    "##### Perplexity:\n",
    "* Perplexity scores (3.12202, 3.64469, 3.94740) suggest that the model is fairly good at predicting the next word, but there is still room for improvement in making the text more fluent and smooth.\n",
    "##### Entropy:\n",
    "* Values ranges from 4.38 to 4.43,indicating that the model is producing text with moderate diversity. The average entropy of 4.4068 suggests that the text has a balanced amount of variation, without being overly random or repetitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d6c44-eae0-4e13-bfaf-afc661772a4b",
   "metadata": {},
   "source": [
    "### Diversity and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c521b9-dfa9-496b-ae25-bf15bafe754c",
   "metadata": {},
   "source": [
    "In the context of poem generation, diversity and variety refer to the richness, uniqueness, and creativity of the generated text, which are critical for producing compelling, engaging, and original poetry.\n",
    "\n",
    "* Self-BLEU : Evaluates how much overlap exists between different samples generated by the same model.\n",
    "* Distinct n-grams: Measures the percentage of distinct n-grams in the generated text compared to the total n-grams. Higher distinctness means more diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4476ffe5-d148-4efa-b2f0-d9048f3f89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import collections\n",
    "\n",
    "# Self-BLEU Score\n",
    "def self_bleu(generated_texts, n=1):\n",
    "    \"\"\"\n",
    "    Calculates the Self-BLEU score for a list of generated texts.\n",
    "    Self-BLEU measures the similarity between different generated outputs,\n",
    "    highlighting repetition or lack of diversity in generation.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list of str): A list of generated text strings to evaluate.\n",
    "        n (int): The n-gram size to use for computation. Default is 1 (unigrams).\n",
    "\n",
    "    Returns:\n",
    "        float: The average Self-BLEU score across all generated texts.\n",
    "    \"\"\"\n",
    "    all_ngrams = [] \n",
    "    for text in generated_texts:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) \n",
    "        tokens = text.split() \n",
    "        ngrams_list = list(ngrams(tokens, n)) \n",
    "        all_ngrams.append(collections.Counter(ngrams_list))\n",
    "    \n",
    "    # Initialize a score to compute Self-BLEU\n",
    "    score = 0\n",
    "    for i, counter in enumerate(all_ngrams):\n",
    "        # Exclude the current text and focus only on other generated outputs\n",
    "        others = all_ngrams[:i] + all_ngrams[i+1:]\n",
    "        union_ngrams = collections.Counter() \n",
    "        for other in others:\n",
    "            union_ngrams.update(other)\n",
    "\n",
    "        # Avoid division by zero by checking if n-grams are available\n",
    "        if sum(union_ngrams.values()) > 0:\n",
    "\n",
    "            overlap = sum(min(count, union_ngrams[gram]) for gram, count in counter.items())\n",
    "            score += overlap / sum(union_ngrams.values())\n",
    "    \n",
    "    return score / len(generated_texts) if len(generated_texts) > 1 else 0\n",
    "\n",
    "\n",
    "# Distinct-N Score\n",
    "def distinct_n(generated_texts, n=2):\n",
    "    \"\"\"\n",
    "    Calculates the Distinct-N score for a list of generated texts.\n",
    "    Distinct-N measures diversity by computing the ratio of unique n-grams\n",
    "    over the total number of n-grams across all generated texts.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list of str): A list of generated text strings to evaluate.\n",
    "        n (int): The n-gram size to compute for Distinct-N. Default is 2 (bigrams).\n",
    "\n",
    "    Returns:\n",
    "        float: The Distinct-N score indicating diversity of generated texts.\n",
    "    \"\"\"\n",
    "    ngrams_set = set()  \n",
    "    total_ngrams = 0 \n",
    "\n",
    "    for text in generated_texts:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        tokens = text.split()\n",
    "        ngrams_list = list(ngrams(tokens, n))  \n",
    "        ngrams_set.update(ngrams_list)\n",
    "        total_ngrams += len(ngrams_list)\n",
    "\n",
    "    return len(ngrams_set) / total_ngrams if total_ngrams > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ce008ecf-9acb-49f6-9143-f1b1d2ba16ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for Model: Gemma 2 2B Q4\n",
      "           Model  Prompt  Self-BLEU (n=1)  Distinct-2\n",
      "0  Gemma 2 2B Q4       1           0.1878     0.95886\n",
      "1  Gemma 2 2B Q4       2           0.1878     0.95886\n",
      "2  Gemma 2 2B Q4       3           0.1878     0.95886\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate Self-BLEU and Distinct-N for a model\n",
    "def evaluate_model_metrics(data, model_name_to_evaluate):\n",
    "    results = []\n",
    "\n",
    "    # Collect all poems generated by the same model across different prompts\n",
    "    generated_poems = data[model_name_to_evaluate].tolist()\n",
    "    self_bleu_score = self_bleu(generated_poems, n=1)  # For unigrams (Self-BLEU n=1)\n",
    "    distinct_2_score = distinct_n(generated_poems, n=2)  # For distinct-2 (bigrams)\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row[\"PromptID\"]\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name_to_evaluate,\n",
    "            \"Prompt\": prompt,\n",
    "            \"Self-BLEU (n=1)\": round(self_bleu_score, 5),\n",
    "            \"Distinct-2\": round(distinct_2_score, 5)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    evaluation_title = f\"Evaluation Results for Model: {model_name_to_evaluate}\"\n",
    "    \n",
    "    print(evaluation_title)\n",
    "    print(results_df)\n",
    "\n",
    "evaluate_model_metrics(data, model_name_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e9eac-2d9a-4254-bb93-c23a03f775bc",
   "metadata": {},
   "source": [
    "##### Self-BLEU (n=1):\n",
    "* The Self-BLEU score of 0.1878 indicates low overlap in unigrams across the texts generated by the model.\n",
    "* This suggests a relatively high level of diversity in the content of the generated poems.\n",
    "##### Distinct-2 Score:\n",
    "* The Distinct-2 score of 0.95886 reflects a high proportion of unique bigrams in the generated poems.\n",
    "* This demonstrates that the model maintains substantial diversity in the construction of phrases and avoids repetitive patterns.\n",
    "##### Consistency Across Prompts:\n",
    "* The scores remain identical for all three prompts.\n",
    "* This implies the model consistently generates diverse outputs regardless of the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bb217-2e49-4b18-8809-35eb87a106a1",
   "metadata": {},
   "source": [
    "### Poetic Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8eb11-b86d-4e55-bb8c-a791f51a8591",
   "metadata": {},
   "source": [
    "A Shakespearean Sonnet is a 14 line poem which follows a consitent rhyme scheme pattern of ABAB CDCD EFEF GG and each line is 10 syllables long written in iambic pentameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2b234-d850-46a4-9c99-bc6727431f77",
   "metadata": {},
   "source": [
    "#### Cleaning the sonnet and storing as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "844c57a8-29eb-42e5-81fe-390432d8dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_poem(poem):\n",
    "    lines = [\n",
    "        line.translate(str.maketrans('', '', string.punctuation))\n",
    "        for line in poem.strip().split('\\n') if line.strip()\n",
    "    ]\n",
    "    \n",
    "    return poem, lines\n",
    "\n",
    "poem1, lines1 = process_poem(poem1)\n",
    "poem2, lines2 = process_poem(poem2)\n",
    "poem3, lines3 = process_poem(poem3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b85a7e-5f4a-4b39-8b32-85bbeaa51214",
   "metadata": {},
   "source": [
    "#### Finding the Rhyming Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0b51525d-fb75-4c76-9c0d-35946fb92e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhyming Scheme for Poem 1: \n",
      "ABAB\n",
      "CDCD\n",
      "EFEF\n",
      "GH\n",
      "Rhyming Scheme for Poem 2: \n",
      "ABAB\n",
      "CDCD\n",
      "EFEF\n",
      "GH\n",
      "Rhyming Scheme for Poem 3: \n",
      "ABAA\n",
      "CDCD\n",
      "EAEF\n",
      "GG\n"
     ]
    }
   ],
   "source": [
    "import pronouncing\n",
    "\n",
    "poem_list = [lines1, lines2, lines3]\n",
    "\n",
    "def get_rhyming_scheme(lines):\n",
    "    last_words = [line.split()[-1] for line in lines]\n",
    "    rhyme_dict = {}\n",
    "    scheme = []\n",
    "    current_letter = 'A'\n",
    "\n",
    "    for word in last_words:\n",
    "        rhymes = pronouncing.rhymes(word)\n",
    "        for key, letter in rhyme_dict.items():\n",
    "            if word in rhymes or key in rhymes:\n",
    "                scheme.append(letter)\n",
    "                break\n",
    "        else:\n",
    "            rhyme_dict[word] = current_letter\n",
    "            scheme.append(current_letter)\n",
    "            current_letter = chr(ord(current_letter) + 1)\n",
    "\n",
    "    rhyme_scheme = ''.join(scheme)\n",
    "    return '\\n'.join([rhyme_scheme[i:i+4] for i in range(0, len(rhyme_scheme), 4)])\n",
    "\n",
    "\n",
    "# Process each poem and print rhyming schemes\n",
    "for i, poem in enumerate(poem_list, 1):\n",
    "    rhyme_scheme = get_rhyming_scheme(poem)\n",
    "    print(f\"Rhyming Scheme for Poem {i}: \\n{rhyme_scheme}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b99450-562e-463a-b43b-3f87122a122f",
   "metadata": {},
   "source": [
    "##### Poem 1 and Poem 2: (ABAB CDCD EFEF GH)\n",
    "* Both deviate in the final rhyming couplet, ending with GH instead of the required GG.\n",
    "##### Poem 3: (ABAA CDCD EAEF GG)\n",
    "* Deviates significantly by breaking the rhyme pattern in the first quatrain (ABAA) and altering the third quatrain (EAEF). However, it maintains the correct final rhyming couplet (GG).\n",
    "\n",
    "These deviations suggest that while the poems attempt structural similarity to a Shakespearean sonnet, they fail to fully conform to its rhyme scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076d926-247f-4163-a279-6f317c8c3216",
   "metadata": {},
   "source": [
    "#### Finding the Syllable Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b0eab1a5-cf5c-4f2d-9cb8-203119944be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem 1 syllable counts: [10, 10, 10, 9, 10, 11, 10, 10, 10, 9, 10, 10, 10, 12]\n",
      "Poem 2 syllable counts: [10, 10, 10, 9, 10, 10, 10, 10, 10, 10, 9, 10, 10, 12]\n",
      "Poem 3 syllable counts: [9, 10, 9, 10, 9, 10, 11, 10, 10, 9, 10, 10, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"\n",
    "    Count syllables for a given word using the CMU Pronouncing Dictionary.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in cmu_dict:\n",
    "        return min([len([y for y in x if y[-1].isdigit()]) for x in cmu_dict[word]])\n",
    "    return 1  \n",
    "\n",
    "def analyze_syllable_counts(poem):\n",
    "    \"\"\"\n",
    "    Analyze the syllable count for each line in the poem.\n",
    "    \"\"\"\n",
    "    syllable_counts = [sum(count_syllables(word) for word in re.findall(r'\\w+', line)) for line in poem]\n",
    "    return syllable_counts\n",
    "\n",
    "all_syllable_counts = [analyze_syllable_counts(poem) for poem in poem_list]\n",
    "\n",
    "for idx, syllable_count in enumerate(all_syllable_counts):\n",
    "    print(f\"Poem {idx + 1} syllable counts: {syllable_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc96085-1e47-47c8-a292-1cd3283e524f",
   "metadata": {},
   "source": [
    "##### Poem 1:\n",
    "* The majority of the lines adhere to the required 10-syllable structure of a Shakespearean sonnet.\n",
    "* However, there are deviations: Lines 4 (9 syllables), 6 (11 syllables), and 14 (12 syllables).\n",
    "##### Poem 2:\n",
    "* This poem also follows the 10-syllable requirement for most lines.\n",
    "* Deviations occur in Lines 4 (9 syllables), 11 (9 syllables), and 14 (12 syllables).\n",
    "##### Poem 3:\n",
    "* The syllable counts are more inconsistent compared to Poem 1 and Poem 2.\n",
    "* Deviations occur in Lines 1 (9 syllables), 3 (9 syllables), 5 (9 syllables), 7 (11 syllables), 10 (9 syllables), and 13 (11 syllables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ed7c5-c6df-45ae-bfd8-94c454c512ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
