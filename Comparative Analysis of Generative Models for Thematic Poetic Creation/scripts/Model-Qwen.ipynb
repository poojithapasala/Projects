{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724aafab-0b89-4d24-b7ce-153679a3aaa4",
   "metadata": {},
   "source": [
    "# Poem Generation using Qwen2.5-1.5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f277bb5-d7b2-4b32-a2c2-88806f56a213",
   "metadata": {},
   "source": [
    "### Load Model and Generate Sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38b1e14b-9c99-43ce-9124-73c6f10ae008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.util import ngrams\n",
    "import pronouncing\n",
    "from nltk.corpus import cmudict\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f6b0d0-03f9-474f-bb87-7b1111d2d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5591d5d-e952-4310-a23b-19cd0acd24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem 1:\n",
      " Write a Shakespearean sonnet about courage set in a battlefield with a determined tone. Use vivid imagery to convey strength and resilience. Amidst the clash of steel and blood, I stand upon the field,\n",
      "A warrior bold who dares not yield his goal.\n",
      "With every stroke my heart beats strong and true,\n",
      "As battle rages on without a pause.\n",
      "\n",
      "The sun is but a beacon far away,\n",
      "Its light a guiding star for weary men.\n",
      "My shield is armor forged from ancient stone,\n",
      "And sword sharp as the wind's relentless blow.\n",
      "\n",
      "In this grim place where life and death are fought,\n",
      "I fight the foe with valor unconfined.\n",
      "For victory begets a better fate,\n",
      "And through my will I conquer every night.\n",
      "\n",
      "Thus, let me speak with voice resounding loud,\n",
      "Of bravery shining bright like morning gold!\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the poem\n",
    "input_text = \"Write a Shakespearean sonnet about courage set in a battlefield with a determined tone. Use vivid imagery to convey strength and resilience.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate poem\n",
    "outputs = model.generate(**input_ids, max_new_tokens=350, temperature=0.7, top_p=0.9)\n",
    "generated_poem1 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Poem 1:\\n\", generated_poem1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f2f6255-fc0e-4d25-b845-ba42307a7650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem 2:\n",
      " Write a Shakespearean sonnet about wonder set in outer space with an awe-filled tone. Use vivid imagery to convey mystery and discovery. In the vast expanse of cosmic air, where stars do twinkle bright,\n",
      "And planets spin with dizzying speed above,\n",
      "A human spirit stirs beneath the night,\n",
      "Where dreams are born on nebulae's ghostly wings.\n",
      "\n",
      "The moon, a silvery orb, ascends aloft,\n",
      "Its light a beacon through the velvet sky,\n",
      "Guiding lost souls into realms unknown,\n",
      "As if the universe were whispering words.\n",
      "\n",
      "Yet here below, among the lunar dust,\n",
      "There looms a question that no answer gives:\n",
      "What secrets lie beyond this endless space?\n",
      "Are we mere specks in cosmic ballet?\n",
      "\n",
      "Oh, how I yearn for worlds unseen!\n",
      "For wonders yet untold, unexplored deep.\n",
      "In this celestial realm, my heart does sing\n",
      "To mysteries so vast, so profound, so wide. \n",
      "\n",
      "So let us venture forth, our spirits free,\n",
      "To explore these wonders, know them all,\n",
      "For there is beauty in the unknown still,\n",
      "And courage in the face of cosmic might.  \n",
      "\n",
      "Thus ends my sonnet, penned by thee,\n",
      "An ode to wonder in outer space,\n",
      "Where minds may soar, hearts expand,\n",
      "And every star a new beginning.  \n",
      "\n",
      "This poem is not only a tribute to the wonder and beauty of space exploration but also reflects the awe-inspiring nature of scientific discovery and the limitless potential for human imagination and curiosity. The sonnet structure allows for a formal rhyme scheme (ababcdcdefefgg) while exploring complex ideas about the cosmos and the role of humanity within it. The use of metaphors like \"nebulae's ghostly wings\" and \"cosmic ballet\" adds depth and poetic flair. The concluding lines express the desire for further exploration and understanding as well as the recognition\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the poem\n",
    "input_text = \"Write a Shakespearean sonnet about wonder set in outer space with an awe-filled tone. Use vivid imagery to convey mystery and discovery.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate poem\n",
    "outputs = model.generate(**input_ids, max_new_tokens=350, temperature=0.7, top_p=0.9)\n",
    "generated_poem2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Poem 2:\\n\", generated_poem2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "158d7423-91eb-4185-8926-20486ddd3b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem 3:\n",
      " Write a Shakespearean sonnet about loss set in an empty home with a somber tone. Use vivid imagery to convey grief and reflection. In silent halls where shadows loom, \n",
      "The echoes of the past remain;  \n",
      "Each room a chamber once where laughter danced,\n",
      "Now quiet as the whispers of the dead.\n",
      "\n",
      "The fire, once bright and warm, now cold and stark,\n",
      "Leaves embers that tell tales long forgotten;\n",
      "A ghostly light flickers from within,\n",
      "A testament to life's fleeting grace.\n",
      "\n",
      "The dust on walls and furniture old,\n",
      "As if time itself had paused its course,\n",
      "In this deserted house, where love did grow,\n",
      "And dreams were woven into every thread.\n",
      "\n",
      "Yet here I stand, alone and lost,\n",
      "To ponder truths that death has taught me most.\n",
      "For though the world may change around us,\n",
      "This place remains, a mirror to my soul. \n",
      "\n",
      "So let the memories come and go,\n",
      "Like autumn leaves upon the ground below;\n",
      "For even when they pass, their beauty stays,\n",
      "In the heart where sorrow holds its sway. \n",
      "\n",
      "Thus ends my poem, written in verse,\n",
      "Of a home left empty, but not forlorn. \n",
      "Here lies a piece of poetry, true,\n",
      "In silence, it tells stories anew. \n",
      "\n",
      "--- Author unknown\n",
      "\n",
      "Note: This is a fictional version of a Shakespearean sonnet. It was inspired by the concept of an empty home setting and the themes of loss and reflection. The original text contains no known reference to any specific author or work. It's meant to be a creative interpretation rather than a direct quote from any existing source. If you have questions regarding the content or style, feel free to ask! I'd be happy to clarify anything you might need. --- \n",
      "\n",
      "---\n",
      "\n",
      "**Author:** A fictional poet who draws inspiration from various sources including Shakespeare's works but creates something entirely new, incorporating elements of the given\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the poem\n",
    "input_text = \"Write a Shakespearean sonnet about loss set in an empty home with a somber tone. Use vivid imagery to convey grief and reflection.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate poem\n",
    "outputs = model.generate(**input_ids, max_new_tokens=350, temperature=0.7, top_p=0.9)\n",
    "generated_poem3 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Poem 3:\\n\", generated_poem3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0bb8a-a8a9-42b4-ad33-3b4f6c9410a2",
   "metadata": {},
   "source": [
    "#### Extracting the necessary content (sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498dd4cf-8ce9-472c-a962-9598c9711b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Poem_Data.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad3b693d-8bcb-4d82-bf62-d091987de8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poem1 = data.loc[0, 'QWEN2 7B Instruct Q4']\n",
    "poem2 = data.loc[1, 'QWEN2 7B Instruct Q4']\n",
    "poem3 = data.loc[2, 'QWEN2 7B Instruct Q4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e41bc5-915c-4f13-9649-c36b3ae48500",
   "metadata": {},
   "source": [
    "#### BLEU (Bilingual Evaluation Understudy)\n",
    "##### What it does: \n",
    "BLEU measures the precision of n-grams (sequences of n consecutive words) in the generated text when compared to a reference text. It’s widely used for machine translation but can also apply to poetry.\n",
    "##### How we’re using it: \n",
    "While BLEU is typically associated with translation, we use it here to evaluate how well the n-grams of the generated poems align with reference poems. This gives us a basic sense of similarity between generated and target structures in terms of word patterns.\n",
    "\n",
    "#### METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "##### What it does: \n",
    "METEOR considers semantic and syntactic similarity by accounting for synonyms, stemming, and word order. This allows it to address some of the weaknesses of the BLEU metric.\n",
    "##### How we’re using it: \n",
    "We use METEOR to assess how closely the generated poem matches the meaning and structure of a reference poem. This is especially helpful for understanding if the model maintains context or theme throughout the text.\n",
    "\n",
    "#### Perplexity\n",
    "##### What it does: \n",
    "Perplexity measures how well a language model can predict the next word in a sequence. A lower perplexity means the model has more confidence in its predictions, which can translate to better fluency.\n",
    "##### How we’re using it: \n",
    "We use perplexity to gauge the fluency and coherence of the generated poems. It gives us an idea of how \"natural\" the poem sounds, even though it doesn’t capture all aspects of creativity.\n",
    "\n",
    "#### Entropy\n",
    "##### What it does: \n",
    "Entropy measures the randomness or diversity of a text by quantifying how varied the token choices are in a given sequence. Higher entropy suggests the model is generating more diverse content, while lower entropy signals repetition or predictability.\n",
    "##### How we’re using it: \n",
    "Entropy is important for understanding how creative or structured the generated poetry is. A balance is key—moderate entropy indicates diversity and creativity, while very low or very high entropy might signal overly repetitive or chaotic outputs. We use this metric to compare different models and analyze their ability to create engaging, varied poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "878fb6ea-a7c2-4240-9117-7ad8ca3f94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "def bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate the BLEU score between the reference and candidate texts.\n",
    "    Measures the overlap of n-grams between the generated text and the reference.\n",
    "    Returns the score rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    return round(sentence_bleu([reference.split()], candidate.split()), 5)\n",
    "\n",
    "def meteor_score_func(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate the METEOR score between the reference and candidate texts.\n",
    "    METEOR measures both exact matches and semantically similar matches \n",
    "    (stemming or synonyms) and penalizes overly short outputs.\n",
    "    Tokenizes both inputs before computing the score.\n",
    "    Returns the score rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    # Tokenize both the reference and candidate\n",
    "    reference_tokens = reference.split()  # Tokenize the reference poem\n",
    "    candidate_tokens = candidate.split()  # Tokenize the generated poem\n",
    "    return round(meteor_score([reference_tokens], candidate_tokens), 5)\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a text using the provided tokenizer and model.\n",
    "    Perplexity is a measure of how well the language model predicts the text.\n",
    "    Lower perplexity indicates higher fluency and alignment with the model's training data.\n",
    "    - Uses the tokenizer to convert the text into input tensors.\n",
    "    - Performs inference using the model with no gradient calculation (for efficiency).\n",
    "    - Computes the loss from the model's output and converts it to perplexity.\n",
    "    Returns the perplexity value rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")  # Tokenize text into tensors\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])  # Compute loss\n",
    "        loss = outputs.loss  # Extract loss\n",
    "        return round(torch.exp(loss).item(), 5)  # Convert loss to perplexity\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a text, representing the diversity in its character or word usage.\n",
    "    Entropy quantifies the unpredictability of the text.\n",
    "    - Tokenizes the text\n",
    "    - Counts the occurrences of each token.\n",
    "    - Calculates probabilities for each token and uses them to compute entropy.\n",
    "    Returns the entropy value rounded to 5 decimal places.\n",
    "    \"\"\"\n",
    "    tokens = list(text)  # Tokenize the text at the character level (default)\n",
    "    token_counts = Counter(tokens)  # Count occurrences of each token\n",
    "    total_tokens = len(tokens)  # Total number of tokens\n",
    "    probabilities = [count / total_tokens for count in token_counts.values()]  # Compute probabilities\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities)  # Compute entropy\n",
    "    return round(entropy, 5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "195c3086-eb90-4fe2-8c31-6a196202d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_to_evaluate = \"QWEN2 7B Instruct Q4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b62b4033-44ef-4828-be12-41ed9e8e78f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for Model: QWEN2 7B Instruct Q4\n",
      "   Prompt   METEOR  Perplexity  Entropy\n",
      "0       1  0.10204     9.34963  4.42495\n",
      "1       2  0.12649     7.87969  4.43341\n",
      "2       3  0.10071     7.32215  4.37548\n",
      "\n",
      "Average Entropy: 4.4113\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Scores\n",
    "results = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    prompt = row[\"PromptID\"]\n",
    "    reference_poem = row[\"ReferencePoem\"]\n",
    "    generated_poem = row[model_name_to_evaluate]\n",
    "    \n",
    "    # Calculate scores\n",
    "    #bleu = bleu_score(reference_poem, generated_poem)\n",
    "    meteor = meteor_score_func(reference_poem, generated_poem)\n",
    "    perplexity = calculate_perplexity(generated_poem)\n",
    "    entropy = calculate_entropy(generated_poem)\n",
    "    \n",
    "    # Append to results\n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        #\"BLEU\": bleu,\n",
    "        \"METEOR\": meteor,\n",
    "        \"Perplexity\": perplexity,\n",
    "        \"Entropy\": entropy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "average_entropy = results_df[\"Entropy\"].mean()\n",
    "evaluation_title = f\"Evaluation Results for Model: {model_name_to_evaluate}\"\n",
    "print(evaluation_title)\n",
    "print(results_df)\n",
    "print(f\"\\nAverage Entropy: {average_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68a42a-940b-47d1-b216-59eac6726e36",
   "metadata": {},
   "source": [
    "##### METEOR Scores:\n",
    "* The METEOR scores for the prompts are relatively low (around 0.10–0.13), indicating a low degree of overlap between the generated text and the reference text.\n",
    "\n",
    "##### Perplexity:\n",
    "* Perplexity values range from approximately 7.32 to 9.35. Lower perplexity generally suggests that the model finds the generated text more probable under its learned distribution. While these values aren't excessively high, they suggest the model could benefit from improved coherence and alignment with expected structures.\n",
    "\n",
    "##### Entropy:\n",
    "* The entropy values (~4.42) indicate the diversity of the generated text in terms of character distribution. Higher entropy suggests more diverse outputs, while lower entropy implies repetitive or predictable patterns. The entropy here is moderate, suggesting a balance between diversity and coherence in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5776da2-74d4-4b14-ad05-cae0eeb4ed41",
   "metadata": {},
   "source": [
    "### Diversity and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2125c-3409-4ce6-b194-8e407a89be16",
   "metadata": {},
   "source": [
    "In the context of poem generation, diversity and variety refer to the richness, uniqueness, and creativity of the generated text, which are critical for producing compelling, engaging, and original poetry.\n",
    "\n",
    "* Self-BLEU : Evaluates how much overlap exists between different samples generated by the same model.\n",
    "* Distinct n-grams: Measures the percentage of distinct n-grams in the generated text compared to the total n-grams. Higher distinctness means more diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e0c1ec5-ca8b-4d83-a4b0-284a85802e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-BLEU Score\n",
    "def self_bleu(generated_texts, n=1):\n",
    "    \"\"\"\n",
    "    Calculates the Self-BLEU score for a list of generated texts.\n",
    "    Self-BLEU measures the similarity between different generated outputs,\n",
    "    highlighting repetition or lack of diversity in generation.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list of str): A list of generated text strings to evaluate.\n",
    "        n (int): The n-gram size to use for computation. Default is 1 (unigrams).\n",
    "\n",
    "    Returns:\n",
    "        float: The average Self-BLEU score across all generated texts.\n",
    "    \"\"\"\n",
    "    all_ngrams = [] \n",
    "    for text in generated_texts:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) \n",
    "        tokens = text.split() \n",
    "        ngrams_list = list(ngrams(tokens, n)) \n",
    "        all_ngrams.append(collections.Counter(ngrams_list))\n",
    "    \n",
    "    # Initialize a score to compute Self-BLEU\n",
    "    score = 0\n",
    "    for i, counter in enumerate(all_ngrams):\n",
    "        # Exclude the current text and focus only on other generated outputs\n",
    "        others = all_ngrams[:i] + all_ngrams[i+1:]\n",
    "        union_ngrams = collections.Counter() \n",
    "        for other in others:\n",
    "            union_ngrams.update(other)\n",
    "\n",
    "        # Avoid division by zero by checking if n-grams are available\n",
    "        if sum(union_ngrams.values()) > 0:\n",
    "\n",
    "            overlap = sum(min(count, union_ngrams[gram]) for gram, count in counter.items())\n",
    "            score += overlap / sum(union_ngrams.values())\n",
    "    \n",
    "    return score / len(generated_texts) if len(generated_texts) > 1 else 0\n",
    "\n",
    "\n",
    "# Distinct-N Score\n",
    "def distinct_n(generated_texts, n=2):\n",
    "    \"\"\"\n",
    "    Calculates the Distinct-N score for a list of generated texts.\n",
    "    Distinct-N measures diversity by computing the ratio of unique n-grams\n",
    "    over the total number of n-grams across all generated texts.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list of str): A list of generated text strings to evaluate.\n",
    "        n (int): The n-gram size to compute for Distinct-N. Default is 2 (bigrams).\n",
    "\n",
    "    Returns:\n",
    "        float: The Distinct-N score indicating diversity of generated texts.\n",
    "    \"\"\"\n",
    "    ngrams_set = set()  \n",
    "    total_ngrams = 0 \n",
    "\n",
    "    for text in generated_texts:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        tokens = text.split()\n",
    "        ngrams_list = list(ngrams(tokens, n))  \n",
    "        ngrams_set.update(ngrams_list)\n",
    "        total_ngrams += len(ngrams_list)\n",
    "\n",
    "    return len(ngrams_set) / total_ngrams if total_ngrams > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "199ee658-9299-4687-86ae-3a148656a82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for Model: QWEN2 7B Instruct Q4\n",
      "                  Model  Prompt  Self-BLEU (n=1)  Distinct-2\n",
      "0  QWEN2 7B Instruct Q4       1          0.18863     0.96352\n",
      "1  QWEN2 7B Instruct Q4       2          0.18863     0.96352\n",
      "2  QWEN2 7B Instruct Q4       3          0.18863     0.96352\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate Self-BLEU and Distinct-N for a model\n",
    "def evaluate_model_metrics(data, model_name_to_evaluate):\n",
    "    results = []\n",
    "\n",
    "    # Collect all poems generated by the same model across different prompts\n",
    "    generated_poems = data[model_name_to_evaluate].tolist()\n",
    "    self_bleu_score = self_bleu(generated_poems, n=1)  # For unigrams (Self-BLEU n=1)\n",
    "    distinct_2_score = distinct_n(generated_poems, n=2)  # For distinct-2 (bigrams)\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row[\"PromptID\"]\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name_to_evaluate,\n",
    "            \"Prompt\": prompt,\n",
    "            \"Self-BLEU (n=1)\": round(self_bleu_score, 5),\n",
    "            \"Distinct-2\": round(distinct_2_score, 5)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    evaluation_title = f\"Evaluation Results for Model: {model_name_to_evaluate}\"\n",
    "    \n",
    "    print(evaluation_title)\n",
    "    print(results_df)\n",
    "\n",
    "evaluate_model_metrics(data, model_name_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91ac1e-477d-4e8e-a443-51b19f401ec9",
   "metadata": {},
   "source": [
    "##### Self-BLEU (n=1):\n",
    "* The Self-BLEU score of 0.1886 indicates low overlap in unigrams across the texts generated by the model.\n",
    "* This suggests a relatively high level of diversity in the content of the generated poems.\n",
    "##### Distinct-2 Score:\n",
    "* The Distinct-2 score of 0.9635 reflects a high proportion of unique bigrams in the generated poems.\n",
    "* This demonstrates that the model maintains substantial diversity in the construction of phrases and avoids repetitive patterns.\n",
    "##### Consistency Across Prompts:\n",
    "* The scores remain identical for all three prompts.\n",
    "* This implies the model consistently generates diverse outputs regardless of the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485ed93-0d33-4269-ac54-d8c82b93f026",
   "metadata": {},
   "source": [
    "### Poetic Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288f9e5-5a3c-44b6-9e11-91a52afe9f41",
   "metadata": {},
   "source": [
    "A Shakespearean Sonnet is a 14 line poem which follows a consitent rhyme scheme pattern of ABAB CDCD EFEF GG and each line is 10 syllables long written in iambic pentameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3f8d1-2d67-4b73-9c6d-b2f9c4ee397a",
   "metadata": {},
   "source": [
    "#### Cleaning the sonnet and storing as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "889b66ac-67d6-4003-b307-e3231e0f78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_poem(poem):\n",
    "    # Clean punctuation and split lines\n",
    "    lines = [\n",
    "        line.translate(str.maketrans('', '', string.punctuation))\n",
    "        for line in poem.strip().split('\\n') if line.strip()\n",
    "    ]\n",
    "    \n",
    "    return poem, lines\n",
    "\n",
    "# Process poem1\n",
    "poem1, lines1 = process_poem(poem1)\n",
    "poem2, lines2 = process_poem(poem2)\n",
    "poem3, lines3 = process_poem(poem3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00887eda-aced-43a7-bd50-8aa2a2fd0a33",
   "metadata": {},
   "source": [
    "#### Finding the Rhyming Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d8cfde1-9b42-487e-a34c-67a360c6f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhyming Scheme for Poem 1: \n",
      "ABCD\n",
      "EFGH\n",
      "IJKL\n",
      "MN\n",
      "Rhyming Scheme for Poem 2: \n",
      "ABAC\n",
      "DEFG\n",
      "HIJK\n",
      "LMNO\n",
      "PQRA\n",
      "PSTU\n",
      "Rhyming Scheme for Poem 3: \n",
      "ABCD\n",
      "EFGH\n",
      "IJKD\n",
      "LMNO\n",
      "KKPQ\n",
      "RSTT\n"
     ]
    }
   ],
   "source": [
    "import pronouncing\n",
    "\n",
    "poem_list = [lines1, lines2, lines3]\n",
    "\n",
    "def get_rhyming_scheme(lines):\n",
    "    last_words = [line.split()[-1] for line in lines]\n",
    "    rhyme_dict = {}\n",
    "    scheme = []\n",
    "    current_letter = 'A'\n",
    "\n",
    "    for word in last_words:\n",
    "        rhymes = pronouncing.rhymes(word)\n",
    "        for key, letter in rhyme_dict.items():\n",
    "            if word in rhymes or key in rhymes:\n",
    "                scheme.append(letter)\n",
    "                break\n",
    "        else:\n",
    "            rhyme_dict[word] = current_letter\n",
    "            scheme.append(current_letter)\n",
    "            current_letter = chr(ord(current_letter) + 1)\n",
    "\n",
    "    rhyme_scheme = ''.join(scheme)\n",
    "    return '\\n'.join([rhyme_scheme[i:i+4] for i in range(0, len(rhyme_scheme), 4)])\n",
    "\n",
    "\n",
    "# Process each poem and print rhyming schemes\n",
    "for i, poem in enumerate(poem_list, 1):\n",
    "    rhyme_scheme = get_rhyming_scheme(poem)\n",
    "    print(f\"Rhyming Scheme for Poem {i}: \\n{rhyme_scheme}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613b682-64d0-42de-b68a-8414dd5ca4ae",
   "metadata": {},
   "source": [
    "##### Poem 1:\n",
    "* The rhyming scheme is straightforward and unique for each line in most stanzas (ABCD, EFGH, IJKL), with the final stanza (MN) suggesting an abrupt end or a shorter stanza.\n",
    "* The lack of repetition in the rhyme scheme implies the model is not adhering to traditional poetic norms.\n",
    "\n",
    "##### Poem 2:\n",
    "* There are sporadic rhyming pairs (like \"A\" in ABAC and \"R\" in PQRA), but the structure is not cohesive. The long and irregular scheme, particularly with unrelated rhymes in consecutive stanzas, suggests that the model generates rhymes without maintaining a global pattern.\n",
    "\n",
    "##### Poem 3:\n",
    "* The inclusion of repeated letters like D and K shows some attempt at rhyme, but it does not adhere to the structural discipline of sonnet forms.\n",
    "\n",
    "The model lacks structural consistency and generates poems having inconsistent rhyme repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20f0d6fd-e9d4-469b-8647-5b1374e706d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\ual-\n",
      "[nltk_data]     laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem 1 syllable counts: [14, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 10, 10, 11]\n",
      "Poem 2 syllable counts: [15, 11, 10, 9, 11, 10, 9, 11, 10, 10, 10, 9, 8, 10, 10, 11, 10, 9, 10, 10, 8, 9, 7, 9]\n",
      "Poem 3 syllable counts: [8, 8, 10, 10, 10, 10, 9, 9, 9, 9, 10, 10, 8, 10, 9, 10, 9, 10, 10, 9, 9, 10, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "nltk.download(\"cmudict\")\n",
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"\n",
    "    Count syllables for a given word using the CMU Pronouncing Dictionary.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in cmu_dict:\n",
    "        return min([len([y for y in x if y[-1].isdigit()]) for x in cmu_dict[word]])\n",
    "    return 1  \n",
    "\n",
    "def analyze_syllable_counts(poem):\n",
    "    \"\"\"\n",
    "    Analyze the syllable count for each line in the poem.\n",
    "    \"\"\"\n",
    "    syllable_counts = [sum(count_syllables(word) for word in re.findall(r'\\w+', line)) for line in poem]\n",
    "    return syllable_counts\n",
    "\n",
    "all_syllable_counts = [analyze_syllable_counts(poem) for poem in poem_list]\n",
    "\n",
    "for idx, syllable_count in enumerate(all_syllable_counts):\n",
    "    print(f\"Poem {idx + 1} syllable counts: {syllable_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c8eab-ef5e-4e3e-b0cd-1d1a6d2277bc",
   "metadata": {},
   "source": [
    "##### Inconsistency in Meter:\n",
    "* The generated poems exhibit partial alignment with metrical norms but fail to consistently follow traditional forms like the iambic pentameter (10 syllables per line).\n",
    "* Irregularities in syllable counts break the rhythm, suggesting the model does not prioritize metrical constraints.\n",
    "##### Poetic Creativity Over Structure:\n",
    "* The syllable counts indicate that QWEN prioritizes creative output over strict adherence to poetic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc4a58-317b-4075-a08c-9eed72e8bac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
